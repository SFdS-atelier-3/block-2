{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "While supervised learning is learning how to associate an outcome variable $y$, for example an image class, to an input $x$, which means that we are learning a function\n",
    "\n",
    "$$y_{pred} = f(x),$$\n",
    "\n",
    "unsupervised learning does not have any target to learn. It is more about *understanding the structure of the data* $\\mathcal D$.\n",
    "\n",
    "Unsupervised learning is a rather vast field, which is hard to summarize mathematically. One thing that most supervised methods have in common is that they, either explicitly, or implicitly, attempt to find a model that maximizes the probability of the data. One can write this as\n",
    "\n",
    "$$\\textrm{maximize } \\mathbb E_{x\\sim\\mathcal D}\\log p_\\vartheta(x),$$\n",
    "\n",
    "where $p_\\vartheta$ is some probabilistic model of the data, $\\mathcal D\\subset\\mathbb R^n$.\n",
    "\n",
    "Note that maximizing the probability of the data under the model should entail that the probability assigned by the model to regions outside the data will be kept as minimal as possible by the model.\n",
    "\n",
    "In this notebook we will use `numpy` and `scikit-learn` to investigate classical methods of unsupervised learning.\n",
    "\n",
    "We will analyze **clustering** methods and **subspace** methods of representing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering methods\n",
    "\n",
    "\n",
    "In general, clustering can be seen as finding a grouping/partition of all available data points such that data points within the same group are (by some metric) similar to each other.\n",
    "\n",
    "<img src=\"images/clustering_methods.png\" alt=\"different clustering methods. image credit: https://scikit-learn.org/stable/modules/clustering.html\"></img>\n",
    "(Find this image in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/clustering.html))\n",
    "\n",
    "One archetype of clustering methods is called K-means, and we will discover it here before moving on to scikit-learn implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Family\n",
    "If you know or assume that your data points are clustered around a known or unknown number of *centers*, then K-means might be a good option to summarize the data.\n",
    "\n",
    "One notion of *being clustered around a number of centers* is the finding that the distances of cluster points to their cluster centers are **much** lower than the distances of the points to the center of all the data points. One must be very careful with this notion, though, because the more cluster centers one adds, the lower the distance of each point to its cluster center will become and eventually, the distances will be 0 when each point is its own cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means\n",
    "\n",
    "To get an idea of what K-means does, let's generate some clustered Gaussian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_samples = 1000\n",
    "K = 5\n",
    "dim = 2\n",
    "centers = np.random.randn(K, dim)\n",
    "affiliations = np.random.randint(0, K, n_samples)\n",
    "noise = np.random.randn(n_samples, dim) * .2\n",
    "\n",
    "X = noise + centers[affiliations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `KMeans` estimator to analyze our data with the correct number of clusters specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=K)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the clusters in different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(K):\n",
    "    cluster = X[kmeans.labels_ == i] \n",
    "    plt.plot(*cluster.T, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means theory\n",
    "\n",
    "Let's take a look a K-means specifically. Assume that we have a set of $n$ observations, $x_1, \\dots, x_n$. We would like to assign each one of them to one of $k$ sets $S_1,\\dots,S_k$, such that each $x_j$ is in exactly one $S_i$, its *cluster*.\n",
    "\n",
    "Let's call $\\mu_i$ the *center* of each cluster $S_i$. Then we can find the distances of each point in the cluster to this center, and sum them up, to get a sort of \"amount of spread-out-ness\":\n",
    "$$\\rho_i = \\sum_{n\\in S_i} d(x_n, \\mu_i)^2$$\n",
    "\n",
    "If we add up the spread-out-ness of all the clusters, we arrive at\n",
    "$$\\rho = \\sum_{i=1}^k\\rho_i = \\sum_{i=1}^k \\sum_{n\\in S_i} d(x_n, \\mu_i)^2$$\n",
    "\n",
    "This formula is called the **K-means objective**. In its typical form, we would use the Euclidean distance $d(x, y) = \\|x - y\\|$, and with that the formula becomes\n",
    "$$\\rho = \\sum_{i=1}^k \\sum_{n\\in S_i} \\|x_n - \\mu_i\\|^2$$\n",
    "\n",
    "**Finding a K-means clustering**\n",
    "We would now like to find 1) Cluster centers $\\mu_i$ and 2) Cluster associations $S_i$.\n",
    "\n",
    "How could we go about doing this? Let us break down the problem a bit, into two questions:\n",
    "\n",
    "**(a)** If we have the cluster centers, can we figure out which data point goes in which cluster?\n",
    "\n",
    "**(b)** If we have the cluster associations, how would we choose the best center?\n",
    "\n",
    "It turns out that we can quite straightforwardly answer both of these questions:\n",
    "\n",
    "**(a)** Let's assume that we know the cluster centers $\\mu_i$. To which cluster should we associate $x$ in order to minimize $\\rho$? Well, probably to the closest one:\n",
    "$$\\arg\\min_i d(\\mu_i, x) $$\n",
    "**(b)** Now let's assume that we know all the members associated with cluster $S_i$. How do we choose its center $\\mu_i$? Well, how about the point that minimizes the sum of all distances to it, i.e. the spread-out-ness $\\rho_i$:\n",
    "$$\\arg\\min_{\\mu_i} \\rho_i = \\arg\\min_{\\mu_i}\\sum_{x_n\\in S_i}d(x_n, \\mu_i)^2$$\n",
    "\n",
    "When we use the Euclidean norm, we can easily find the exact minimum of this equation:\n",
    "\n",
    "The derivative of $\\|x_n - \\mu_i\\|^2$ is $2(x_n - \\mu_i)$, so the derivative of $\\rho_i$ with respect to $\\mu_i$ is \n",
    "$$\\nabla_{\\mu_i}\\sum_{x_n\\in S_i}\\|x_n - \\mu_i\\|^2 = \\sum_{x_n\\in S_i}2(x_n - \\mu_i) = \\sum_{x_n\\in S_i}2x_n - \\sum_{x_n\\in S_i}2\\mu_i = \\sum_{x_n\\in S_i}2x_n - 2|S_i|\\mu_i$$\n",
    "Setting the derivative to 0, we find that\n",
    "$$\\mu_i = \\frac{1}{|S_i|}\\sum_{x_n\\in S_i}x_n,$$\n",
    "the average of the samples associated with $S_i$.\n",
    "\n",
    "How do we use this to get the  clusters? We have created something of a **chicken and egg problem**: 1 and 2 depend on each other. But where to start?\n",
    "\n",
    "Well, how about we try starting with random assignments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a matter of fact, why don't we split this into an exercise:\n",
    "\n",
    "**Exercise 01:** Implement K-means.\n",
    "\n",
    "Parts **1\\.** and **2\\.** will implement b\n",
    "\n",
    "Parts **3\\.** and **4\\.** will implement a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = np.random.randint(0, K, n_samples)\n",
    "assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.** Above is a random assignment matrix called `assignments`, assigning each data point to one of the clusters. Use it to create a binary matrix of shape `(n_samples, K)` containing a `True` value whenever the `nth` sample is of class `i`. Use broadcasting to create this matrix.  Call it `assignment_matrix`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/unsupervised/solution01.1.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-2/master/solutions/unsupervised/solution01.1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2\\.** Multiplying the `assignment_matrix` to the data in the correct way will obtain sums for each class. Use this to compute the mean of each class. Call them `cluster_centers` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/unsupervised/solution01.2.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-2/master/solutions/unsupervised/solution01.2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.** Now compute the distance of each cluster center you obtained in b) to all the data points. You can use `sklearn.metrics.euclidean_distances` or pure `numpy` for this. \n",
    "\n",
    "This should leave you with a matrix of shape `number of samples, number of clusters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/unsupervised/solution01.3.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-2/master/solutions/unsupervised/solution01.3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4\\.** Use this distance matrix to find the associations, by checking to which cluster center each data point is closest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/unsupervised/solution01.4.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-2/master/solutions/unsupervised/solution01.4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5\\.** Putting it all together: Write a for-loop that iterates through a-d many times to implement kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/unsupervised/solution01.5.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-2/master/solutions/unsupervised/solution01.5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6\\.** Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/unsupervised/solution01.6.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-2/master/solutions/unsupervised/solution01.6.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7\\.** Use `sklearn.cluster.KMeans` on `X` and compare visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/unsupervised/solution01.7.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-2/master/solutions/unsupervised/solution01.7.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New data and K-means\n",
    "\n",
    "If new data came in and we asked our K-means to put a label to it, what would it say?\n",
    "\n",
    "To answer this, we can make a 2D grid spanning a relevant region of 2D space and ask which label would be assigned to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xmin, ymin), (xmax, ymax) = X.min(0), X.max(0)\n",
    "grid = np.mgrid[ymin:ymax:0.01, xmin:xmax:0.01]\n",
    "reshaped_grid = grid.reshape(2, -1)[::-1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_labels = kmeans.predict(reshaped_grid).reshape(grid[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(grid_labels)\n",
    "for i in range(K):\n",
    "    cluster = X[labels == i] \n",
    "    #plt.plot(*cluster.T, 'o')\n",
    "    plt.plot(*((cluster - (xmin, ymin)).T * 100), \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This amounts to a so-called Voronoi-tesselation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What if we don't know the number of clusters?\n",
    "\n",
    "One thing we can do is try out different numbers of them. As mentioned before, this comes with risks when evaluating the total amount of distortion. But let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for k in range(1, 21):\n",
    "    km = KMeans(n_clusters=k).fit(X)\n",
    "    labels = km.predict(X)\n",
    "    plt.subplot(4, 5, k)\n",
    "    for i in range(k):\n",
    "        plt.plot(*X[labels == i].T, \"o\")\n",
    "    plt.title(f\"K={k}\")\n",
    "    distortions.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(np.arange(1, 21), distortions)\n",
    "plt.xticks(np.arange(1, 21), [f\"{i}\" for i in np.arange(1, 21)])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can indeed tell that the \"correct number of clusters\", 5, is just at the beginning of the long flat range. This plot is sometimes called an \"elbow\".\n",
    "\n",
    "This method is also often criticized, since it is not entirely clear where the elbow starts exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for evaluating K-means clusterings\n",
    "\n",
    "Evaluating clustering methods in the sense of \"how good is this clustering\" is hard and probably not even well definable. It will depend very strongly on why one wanted to do the clustering in the first place. Apart from the \"elbow\" method mentioned above, several methods for evaluating clusterings are described in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation). Unfortunately only one single one of them, the *silhouette coefficient*, does not assume the knowledge of the ground-truth labels.\n",
    "\n",
    "##### Silhouette coefficient\n",
    "The silhouette coefficient is an evaluation criterion that can be computed point-by-point. For every given point in a clustering it compares the mean distance $a$ to all points in the same cluster to the mean distance $b$ to all points in the next-closest cluster by\n",
    "$$s = \\frac{b - a}{\\max(a, b)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score, silhouette_samples\n",
    "\n",
    "silhouette_scores = []\n",
    "silhouette_sample_scores = []\n",
    "for k in range(2, 21):\n",
    "    km = KMeans(n_clusters=k).fit(X)\n",
    "    labels = km.predict(X)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "    silhouette_sample_scores.append(silhouette_samples(X, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(range(2, 21), silhouette_scores)\n",
    "plt.xticks(np.arange(1, 21), [f\"{i}\" for i in np.arange(1, 21)])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "for k, sil_samples in zip(range(2, 21), silhouette_sample_scores):\n",
    "    plt.subplot(4, 5, k)\n",
    "    plt.scatter(*X.T, color=plt.cm.hot(1 - sil_samples))\n",
    "    plt.title(f\"K={k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-medoids\n",
    "There are sometimes situations in which we can evaluate distances between objects, but there is no continuity in the underlying subspace permitting the notion of a \"mean\". In this case it can be of interest to select data points as cluster centers.\n",
    "\n",
    "This is also useful when there are outliers in the data - the mean gets perturbed by this, but the median (which is kind of what we are selecting here) does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel K-means\n",
    "\n",
    "Sometimes it is useful to use kernels with clustering. This amounts to projecting the data points into a higher-dimensional space before the clustering operation. All computations can be performed in the sample space and the high-dimensional kernel space need not be instantiated. Sometimes certain clusters become more apparent in other feature spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subspace-based methods\n",
    "This section collects together methods that assume that there is some continuous subspace of the data space that the data points live in.\n",
    "\n",
    "### Flat subspaces\n",
    "A big class of unsupervised learning algorithms works with linear subspaces. This means that they attempt to construct the data points as linear combinations of some prototypes, which are learned from the data under various constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA - Principle Components Analysis\n",
    "\n",
    "PCA is a well-known and well-studied linear method of dimensionality reduction. It is used and prevalent in many applications and is often a very strong baseline for unsupervised methods.\n",
    "\n",
    "\n",
    "There are several ways of deriving it. One is about finding the directions of maximal variance in a point cloud.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a point cloud\n",
    "dimension = 2\n",
    "n_samples = 200\n",
    "\n",
    "X = np.random.randn(n_samples, dimension)\n",
    "random_rotation, _, _ = np.linalg.svd(np.random.randn(2, 2))\n",
    "random_scaling = np.exp(np.random.randn(2))\n",
    "\n",
    "X = (X * random_scaling).dot(random_rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(*X.T, \"o\")\n",
    "m = np.abs(X).max() * 1.1\n",
    "plt.axis([-m, m, -m, m])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell creates a somewhat sophisticated interactive plot in which you can find the direction of maximum variance by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projection_histogram(angle, offset, X, lines=True):\n",
    "    rad = angle / 180 * np.pi\n",
    "    line_vector = np.array((np.cos(rad), np.sin(rad)))\n",
    "    m = np.linalg.norm(X, axis=1).max() * 1.1\n",
    "    lx, ly = line_vector[:, np.newaxis] * (-m, m)\n",
    "    ly += offset\n",
    "    offset_vector = np.array((0, offset)) - line_vector * line_vector.dot(np.array((0, offset)))\n",
    "    px, py = line_vector[:, np.newaxis] * X.dot(line_vector) + offset_vector[:, np.newaxis]\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(*X.T, \"o\")\n",
    "    plt.plot(lx, ly)\n",
    "    plt.plot(px, py, \".\")\n",
    "    if lines:\n",
    "        xs = np.vstack((X[:, 0], px))\n",
    "        ys = np.vstack((X[:, 1], py))\n",
    "        plt.plot(xs, ys, \"r\", lw=0.5)\n",
    "    plt.axis((-m, m, -m, m))\n",
    "    \n",
    "    projection_location = px * line_vector[0] + py * line_vector[1]\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bins = np.linspace(-1.2 * m, 1.2 * m, X.shape[0] // 10)\n",
    "    plt.hist(projection_location, bins=bins)\n",
    "    plt.title(f\"variance: {np.var(projection_location):0.2f}\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "from functools import partial\n",
    "def plotter(angle, offset):\n",
    "    return plot_projection_histogram(angle, offset, X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, use the sliders to find the direction of maximal variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_slider = FloatSlider(0., min=-95., max=95., step=1.)\n",
    "offset_slider = FloatSlider(0., min=-10, max=10, step=.1)\n",
    "i = interact(plotter, angle=angle_slider, offset=offset_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find them using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `PCA` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit it to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the PCA object is fit to the data, we can query it for different properties of the data.\n",
    "\n",
    "Let's start with explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aligned the above line with the maximal direction of variance, then it should show the value indicated in the first position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can take a look at PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component is the first row of the above matrix and should show a vector parallel with the maximal variance axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_selected_direction = np.array((np.cos(angle_slider.value / 180 * np.pi),\n",
    "                                    np.sin(angle_slider.value / 180 * np.pi)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_[0].dot(hand_selected_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the slider was set to be parallel to the maximum variance axis, this value should be high, close to 1 or -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "faces = fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces['images'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a panel to take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(faces['images'].reshape(20, 20, 64, 64).transpose(0, 2, 1, 3).reshape(20 * 64, -1), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a PCA object for these faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_pca = PCA(n_components=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and fit it to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_pca.fit(faces['images'].reshape(faces['images'].shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the explained variance by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(faces_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there is not much variance left after 5 components. Let's plot the first 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(faces_pca.components_[i].reshape(64, 64), cmap=\"gray\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the different components highlight different parts of the faces.\n",
    "\n",
    "We can also display the mean face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(faces_pca.mean_.reshape(64, 64), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude from this analysis that (aligned) faces live in a *relatively* small subspace as far as PCA is concerned. (Evidently faces can also vary much more that those shown in the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image patches\n",
    "Another informative analysis is to decompose small patches of images, where the exact location of the patch cannot be known. This leads to different types of components.\n",
    "\n",
    "Let's get an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.data import astronaut\n",
    "a = astronaut()\n",
    "\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use a convenience function to extract patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.image import extract_patches_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = extract_patches_2d(a, (8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_pca = PCA(n_components=100).fit(patches.reshape(patches.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and create little component images which we renormalize for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_images = patch_pca.components_.reshape((-1,) + patches.shape[1:])\n",
    "component_images = (component_images - component_images.min(axis=(1,2,3), keepdims=True))\n",
    "component_images = component_images / component_images.reshape(len(component_images), -1).ptp(axis=1).reshape(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we display them as a panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(component_images.reshape(10, 10, 8, 8, 3).transpose(0, 2, 1, 3, 4).reshape(80, 80, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe very regular, predominantly black and white patterns. These are almost exactly the Fourier components of these 8x8 patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICA\n",
    "\n",
    "Instead of searching orthogonal components, which is what PCA does due to its loss, one can attempt to find statistically independent components. ICA is one of these ways. Let's make some data that can make this clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, 2)\n",
    "\n",
    "s = .9\n",
    "shear_mat = np.array([[1, s], [0, 1]])\n",
    "X = X.dot(shear_mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*X.T, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components=2, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*X.T, \"o\")\n",
    "plt.plot([0, ica.components_[0, 0]] * 10, [0, ica.components_[0, 1]] * 10)\n",
    "plt.plot([0, ica.components_[1, 0]] * 10, [0, ica.components_[1, 1]] * 10)\n",
    "plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components are the little lines at the bottom. It becomes clear that they are either aligned with or orthogonal to the data axes. They are exactly at the angle to each other that the data takes to itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the outcome of the transformation that ICA induces, which is to try to make the data as independent as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ica.transform(X).T, \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we generated the data ourselves, we can see what ICA estimates. It actually estimates, up to permutation of axes and orientation, an inverse of the shear matrix used to generate the data! This is exactly why it is able to straighten out the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ica.components_.dot(shear_mat))\n",
    "plt.matshow(ica.components_.dot(shear_mat))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at PCA for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=2).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*X.T, \"o\")\n",
    "plt.plot([0, pca2.components_[0, 0]] * 10, [0, pca2.components_[0, 1]] * 10)\n",
    "plt.plot([0, pca2.components_[1, 0]] * 10, [0, pca2.components_[1, 1]] * 10)\n",
    "plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these directions are orthogonal to each other, they will definitely not be able to undo the transform we applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*pca2.transform(X).T, \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, it just rotates the data, but the sheared shape remains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICA on image patches\n",
    "While PCA applied to image patches finds orthogonal components, and ends of roughly finding Fourier components of the square basis, ICA does not have this orthogonality restriction and is free to find actually statistically independent components.\n",
    "\n",
    "These will be much more localized in space, since a lot of variability in images lies in localized contours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create and ICA object with 100 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_ica = FastICA(n_components=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit is to raveled patch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_ica.fit(patches.reshape(patches.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the components and shape them back into patch shape. Then we subtract minimum and divide by the difference between minimum and maximum (peak-to-peak, `ptp`) in order to bring the patches to the $[0,1]^3$ interval for color display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_component_images = patch_ica.components_.reshape((-1,) + patches.shape[1:])\n",
    "ica_component_images = (ica_component_images - ica_component_images.min(axis=(1,2,3), keepdims=True))\n",
    "ica_component_images = ica_component_images / ica_component_images.reshape(len(ica_component_images), -1).ptp(axis=1).reshape(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next display the patches as a panel through appropriate reshaping and transposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ica_component_images.reshape(10, 10, 8, 8, 3).transpose(0, 2, 1, 3, 4).reshape(80, 80, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICA on faces\n",
    "We can of course also look at the independent components of the faces:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_ica = FastICA(n_components=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_ica.fit(faces['images'].reshape(faces['images'].shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(faces_ica.components_[i].reshape(64, 64), cmap=\"gray\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note:** Find a comprehensive set of decomposition methods applied to faces here https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 dictionary learning\n",
    "\n",
    "$\\ell_1$-penalized dictionary learning attempts to find image components such that an image can be reconstructed using a sparse linear combination of these components. It minimizes the following (biconvex) optimization problem\n",
    "\n",
    "$$\\arg\\min_D \\min_\\alpha \\|X - D\\alpha\\|^2 + \\lambda \\|\\alpha\\|_1,$$\n",
    "\n",
    "where $D$ is a dictionary of template signals\n",
    "\n",
    "**Note:** The implementation of this method is a bit memory-intensive and might cause some machines to crash. It is currently in a setting where the dictionary size is too small to yield sparse activations, which is why the results are not so impressive. This is something very interesting to re-check on a bigger computer though, when you have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = make_pipeline(StandardScaler(), MiniBatchDictionaryLearning(n_components=128, n_iter=500, n_jobs=1, batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.fit(patches.reshape(patches.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.steps[1][1].components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_component_images = dl.steps[1][1].components_.reshape((-1,) + patches.shape[1:])\n",
    "dl_component_images = (dl_component_images - dl_component_images.min(axis=(1,2,3), keepdims=True))\n",
    "dl_component_images = dl_component_images / dl_component_images.reshape(len(dl_component_images), -1).ptp(axis=1).reshape(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dl_component_images[:100].reshape(10, 10, 8, 8, 3).transpose(0, 2, 1, 3, 4).reshape(80, 80, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
